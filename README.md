ğŸ¥ Streaming AI Agent with Gemini
This project demonstrates how to build a streaming AI agent using
Google Gemini models with an OpenAI-compatible API client.
It uses Pythonâ€™s asyncio for handling streaming responses and rich for colorful terminal output.

ğŸš€ Features
âœ… Streaming AI responses in real-time
âœ… Uses Google Gemini 2.0 Flash model
âœ… Async implementation with Pythonâ€™s asyncio
âœ… Pretty printing with rich
âœ… Environment variable support via .env
ğŸ“‚ Project Structure
. â”œâ”€â”€ main.py # Main entry point (async streaming example) â”œâ”€â”€ agents.py # Agent + Runner utilities â”œâ”€â”€ .env # Store your API key (not committed to git) â””â”€â”€ README.md # Project documentation

yaml Copy Edit

âš™ï¸ Installation
Clone the repository
git clone <your-repo-url>
cd <your-repo-name>
Create a virtual environment

bash Copy Edit python -m venv .venv source .venv/bin/activate # Mac/Linux .venv\Scripts\activate # Windows Install dependencies

bash Copy Edit pip install -r requirements.txt Example requirements:

txt Copy Edit openai python-dotenv rich (plus your local agents.py module)

ğŸ”‘ Environment Variables Create a .env file in the project root:

env Copy Edit GEMINI_API_KEY=your_api_key_here â–¶ï¸ Usage Run the script:

bash Copy Edit python main.py If using uv:

bash Copy Edit uv run --no-cache main.py ğŸ–¥ï¸ Example Output When you run the script, youâ€™ll see streaming output like:

bash Copy Edit H He Hel Hell Hello, how can I help you today? (or styled text if using rich).

ğŸ› ï¸ Customization Change the initial prompt in main.py:

python Copy Edit result = Runner.run_streamed(agent, "Hi") ğŸ‘‰ Replace "Hi" with your own query.

Change the model in main.py:

python Copy Edit model=OpenAIChatCompletionsModel(model="gemini-2.0-flash", ...)
